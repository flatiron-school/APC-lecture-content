{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Data - TimeSeries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few strategies we can choose from and they each have their pros/cons:\n",
    "1) **Fill with a Relevant Value**  \n",
    " - If we know what the nulls 'should' be, easy to fill them with that value\",  \n",
    " - For numbers, perhaps the null indicates a 0\",  \n",
    " - Or, for string columns, might be easier to handle if we fill with \\\"Missing\\\" or \\\"Unknown\\\"\n",
    " \n",
    "2) **Fill with a Reasonable Value**   \n",
    " - For numeric data, it might be acceptable to fill with a measure of central tendency (mean or median),\n",
    " - For categorical/string data, might be acceptable to fill with the most common (mode),\n",
    " - But beware! Filling in missing values can lead to you drawing incorrect conclusions. If most of the data from a column are missing, it's going to appear that the value you filled it in with is more common that it actually was!  \n",
    "\n",
    "3) **Specify Missing Data**  \n",
    " - If you plan to fill in missing values, it might make sense to specify that the data was originally missing by creating a new indicator column,\n",
    " - This can be helpful when you suspect that the fact the data was missing could be important for an analysis.\n",
    "\n",
    "4) **Drop Missing Data**  \n",
    " - While you should try to keep as much relevant data as possible, sometimes the other methods don't make as much sense and it's better to remove or **drop** the missing data,\n",
    " - We typically drop missing data if very little data would be lost and/or trying to fill in the values wouldn't make sense for our use case  \n",
    "\n",
    "In order of preference, we go with 1, 3, 2, and 4.\n",
    "\n",
    "### Any time we fill a null value with a mathematical process, we are creating data. This means that any resulting insights need to be reported with this in mind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as st\n",
    "import seaborn as sns\n",
    "\n",
    "pd.set_option('display.max_rows', 5000)\n",
    "plt.rcParams[\"figure.figsize\"] = [10.5, 10.5]\n",
    "plt.rcParams[\"figure.autolayout\"] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apc = pd.read_parquet('./data/Flat_Iron_Dataset_06292023.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apc['sample_date'] = pd.to_datetime(apc['sample_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrkg = apc[apc[\"fldptag\"]==\"2002000001841276\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrkg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrkg = wrkg.set_index('sample_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrkg = wrkg.pivot(columns='endpoint_id', values='voltage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrkg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrkg.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like only one of our customers has a full dataset. Let's take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(wrkg.isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_series = wrkg.iloc[:, 16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_series.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The challenge with `interpolate` is that it can't go from nothing. It needs to have a starting point and an ending point. In other words, it's only effective for gaps in data, not whole cloth creation.\n",
    "\n",
    "The data we've got here, as we know, has enormous gaps in it that begin at the first timestep for almost all of the observations. So we need to create some gaps in our data, and we're going to do that with resampling. Fortunately, there's built-in resampling in pandas.\n",
    "\n",
    "Two types of resampling are:\n",
    "\n",
    "**Upsampling**: Where you increase the frequency of the samples, such as from minutes to seconds.  \n",
    "**Downsampling**: Where you decrease the frequency of the samples, such as from days to months.\n",
    "\n",
    "Our data are in fifteen minute increments. We need to downsample it, and then upsample it in order to create some null values. Since we're downsampling, we also need to provide a way to consolidate the data, using mean in this example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downsample = raw_series.resample(\"D\").mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Rule | Meaning |\n",
    "|--|:--|\n",
    "|B       |  business day frequency|\n",
    "|C      |   custom business day frequency (experimental)|\n",
    "|D     |    calendar day frequency|\n",
    "|W    |     weekly frequency|\n",
    "|M   |      month end frequency|\n",
    "|SM |       semi-month end frequency (15th and end of month)|\n",
    "|BM  |      business month end frequency|\n",
    "|CBM     |  custom business month end frequency|\n",
    "|MS     |   month start frequency|\n",
    "|SMS   |    semi-month start frequency (1st and 15th)|\n",
    "|BMS  |     business month start frequency|\n",
    "|CBMS|      custom business month start frequency|\n",
    "|Q |        quarter end frequency|\n",
    "|BQ|        business quarter endfrequency|\n",
    "|QS|        quarter start frequency|\n",
    "|BQS|       business quarter start frequency|\n",
    "|A|         year end frequency|\n",
    "|BA, BY  |  business year end frequency|\n",
    "|AS, YS  |  year start frequency|\n",
    "|BAS, BYS | business year start frequency|\n",
    "|BH       | business hour frequency|\n",
    "|H        | hourly frequency|\n",
    "|T, min  |  minutely frequency|\n",
    "|S      |   secondly frequency|\n",
    "|L, ms |    milliseconds|\n",
    "|U, us|     microseconds|\n",
    "|N  |       nanoseconds|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downsample.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upsample = downsample.resample(\"H\").asfreq()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upsample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've now got a series witha bunch of nulls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series = upsample.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `interpolate` method in pandas has a truly ridiculous number of options for the `method` parameter:\n",
    "\n",
    "> ‘linear’: Ignore the index and treat the values as equally spaced. This is the only method supported on MultiIndexes.\n",
    "\n",
    "> ‘time’: Works on daily and higher resolution data to interpolate given length of interval.\n",
    "\n",
    "> ‘index’, ‘values’: use the actual numerical values of the index.\n",
    "\n",
    "> ‘pad’: Fill in NaNs using existing values.\n",
    "\n",
    ">‘nearest’, ‘zero’, ‘slinear’, ‘quadratic’, ‘cubic’, ‘barycentric’, ‘polynomial’: Passed to scipy.interpolate.interp1d, whereas ‘spline’ is passed to scipy.interpolate.UnivariateSpline. These methods use the numerical values of the index. Both ‘polynomial’ and ‘spline’ require that you also specify an order (int), e.g. df.interpolate(method='polynomial', order=5). Note that, slinear method in Pandas refers to the Scipy first order spline instead of Pandas first order spline.\n",
    "\n",
    "> ‘krogh’, ‘piecewise_polynomial’, ‘spline’, ‘pchip’, ‘akima’, ‘cubicspline’: Wrappers around the SciPy interpolation methods of similar names. See Notes.\n",
    "\n",
    "> ‘from_derivatives’: Refers to scipy.interpolate.BPoly.from_derivatives which replaces ‘piecewise_polynomial’ interpolation method in scipy 0.18."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_series = series.interpolate(\"linear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Linear` treats the spaces as if they are equal. In this case, they are equal, so there's no difference than with `time` or `index`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_series.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series = series.interpolate(\"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Polynomial` fits a polynomial of order _k_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_series = series.interpolate('polynomial', order=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_series.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Spline` uses spline interpolation, fitting a piecewise polynomial to the intervening space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spline_series = series.interpolate('spline', order=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spline_series.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deriv_series = series.interpolate('from_derivatives')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deriv_series.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realistically, interpolate can do the job pretty well, so long as the job is filling in gaps in the data. It's no good when it comes to creating data from whole cloth. If we want to really fill in from the start, I would suggest the following process:\n",
    "\n",
    "1) Calculate the diffs for the data that you have. `pd.diff` will do the job.  \n",
    "2) Use the diffs to generate a distribution.  \n",
    "3) Use this distribution to generate a vector of random values for the appropriate timestamps, using the mean of the known timeseries as a starting point.  \n",
    "\n",
    "Obviously, this assumes that the missing time series follows the same distribution as the known time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
